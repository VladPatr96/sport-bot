#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Universal Entity News ‚Äî –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Å—É—â–Ω–æ—Å—Ç–∏
(—Å–ø–æ—Ä—Ç / —Ç—É—Ä–Ω–∏—Ä / –∫–æ–º–∞–Ω–¥–∞ / —Å–ø–æ—Ä—Ç—Å–º–µ–Ω) —á–µ—Ä–µ–∑ tag_url ‚Üí tag_id ‚Üí news_ids ‚Üí news.

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
  ‚Ä¢ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä —Ç–∏–ø–∞ —Å—É—â–Ω–æ—Å—Ç–∏ –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–ø–∏—Å–∏ (–ø–æ–∏—Å–∫ –ø–æ –ø–æ–¥—Å—Ç—Ä–æ–∫–µ –∏–ª–∏ ID)
  ‚Ä¢ –£–º–Ω—ã–π –ø–æ–∏—Å–∫ —Ç–µ–≥–∞ –≤ —Ç–∞–±–ª–∏—Ü–µ `tags` (url, –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –ø—É—Ç—å, LIKE –ø–æ —Ö–≤–æ—Å—Ç—É, slug/name)
  ‚Ä¢ –ü–æ–¥—Å–∫–∞–∑–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π/–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–π –≤ –ë–î –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ç–µ–≥–∞
  ‚Ä¢ –í—ã–±–æ—Ä –ø–µ—Ä–∏–æ–¥–∞: –ø—Ä–µ—Å–µ—Ç—ã (6—á/12—á/24—á/3–¥/7–¥/—Å–µ–≥–æ–¥–Ω—è/–≤—á–µ—Ä–∞) –∏–ª–∏ —Ä—É—á–Ω–æ–π FROM/TO
  ‚Ä¢ –í—ã–±–æ—Ä–∫–∞ —Å—Ç–∞—Ç–µ–π –∏–∑ `news` –ø–æ —Å–ø–∏—Å–∫—É `news_id` –∏–∑ `news_article_tags`
  ‚Ä¢ –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π fallback-–ø–∞—Ä—Å–∏–Ω–≥ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ç–µ–≥–∞ (—Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏, —á—Ç–æ–±—ã –Ω–µ –ª–æ–≤–∏—Ç—å 403)
  ‚Ä¢ –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ CSV/JSON
  ‚Ä¢ Windows-safe –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É—Ç–µ–π –∫ –ë–î, –ø–µ—á–∞—Ç—å —Ä–µ–∑–æ–ª–≤–ª–µ–Ω–Ω–æ–≥–æ –ø—É—Ç–∏ –ø—Ä–∏ -v

–û–∂–∏–¥–∞–µ–º–∞—è —Å—Ö–µ–º–∞ (–∏–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫ autodetect):
  - –¢–∞–±–ª–∏—Ü–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ (–∏—â–µ—Ç—Å—è –ø–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º):
      team ‚Üí teams/team; player ‚Üí players/player; tournament ‚Üí tournaments/tournament/leagues/league; sport ‚Üí sports/sport
      –ø–æ–ª—è: id | name/title | tag_url/tag/url
  - tags(id|tag_id, url|href [, slug] [, name/title/label])
  - news_article_tags(news_id|article_id, tag_id|label_id)
  - news(id|news_id, title|headline|name, url|link, published_at|published|pub_date|date|datetime [, source|origin|site])

–ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞:
  python -m scripts.universal_entity_news --db F:/projects/Projects/sport-news-bot/database/prosport.db -v
  python -m scripts.universal_entity_news --db ./database/prosport.db --export csv --export-path ./out/cska.csv
"""
from __future__ import annotations

import os
import re
import sys
import csv
import json
import sqlite3
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, TYPE_CHECKING

# ---- typing-only ZoneInfo type to satisfy Pylance ----
if TYPE_CHECKING:
    from zoneinfo import ZoneInfo as ZoneInfoType  # for annotations only
else:  # runtime placeholder
    class ZoneInfoType:  # type: ignore
        pass

# ---- runtime import (may be None) ----
try:
    from zoneinfo import ZoneInfo as _ZoneInfo  # py>=3.9
except Exception:  # pragma: no cover
    try:
        from backports.zoneinfo import ZoneInfo as _ZoneInfo  # type: ignore
    except Exception:  # pragma: no cover
        _ZoneInfo = None  # type: ignore

try:
    import requests  # optional for fallback parse
except Exception:  # pragma: no cover
    requests = None  # type: ignore

# -----------------------------
# Models
# -----------------------------
@dataclass
class Entity:
    id: int
    name: str
    tag_url: str

@dataclass
class Article:
    id: int
    title: str
    url: str
    published_at: str  # ISO with 'T'
    source: Optional[str]

# -----------------------------
# TZ helpers
# -----------------------------

def get_tz() -> Optional[ZoneInfoType]:
    tz_name = os.environ.get("APP_TZ") or os.environ.get("TZ") or "UTC"
    try:
        return _ZoneInfo(tz_name) if _ZoneInfo else None
    except Exception:
        return None


def parse_dt_local(s: str, tz: Optional[ZoneInfoType]) -> datetime:
    s = s.strip()
    if not s:
        raise ValueError("empty datetime string")
    fmt = "%Y-%m-%d %H:%M" if " " in s else "%Y-%m-%d"
    dt = datetime.strptime(s, fmt)
    return dt.replace(tzinfo=tz) if tz else dt


def to_iso_T(dt: datetime) -> str:
    """Return 'YYYY-MM-DDTHH:MM:SS' in UTC-naive for lexicographic compare in SQLite."""
    if dt.tzinfo is not None and _ZoneInfo:
        dt = dt.astimezone(_ZoneInfo("UTC")).replace(tzinfo=None)
    return dt.isoformat(timespec="seconds")

# -----------------------------
# DB helpers
# -----------------------------

def resolve_db_path(db_arg: str) -> str:
    raw = db_arg.strip()
    if raw.lower().startswith("sqlite:///"):
        raw = raw[10:]
    raw = os.path.expandvars(os.path.expanduser(raw))
    raw = os.path.normpath(raw)
    abs_path = os.path.abspath(raw)
    parent = os.path.dirname(abs_path)
    if parent and not os.path.exists(parent):
        os.makedirs(parent, exist_ok=True)
    return abs_path


def connect_db(db_path: str, verbose: bool = False) -> sqlite3.Connection:
    resolved = resolve_db_path(db_path)
    if verbose:
        print(f"üóÑÔ∏è  DB path resolved to: {resolved}")
    con = sqlite3.connect(resolved)
    con.row_factory = sqlite3.Row
    con.execute("PRAGMA foreign_keys = ON;")
    return con


def table_exists(con: sqlite3.Connection, name: str) -> bool:
    return con.execute("SELECT 1 FROM sqlite_master WHERE type='table' AND name=?", (name,)).fetchone() is not None


def columns(con: sqlite3.Connection, table: str) -> List[str]:
    return [r[1] for r in con.execute(f"PRAGMA table_info({table})").fetchall()]


def pick(cols: List[str], cands: List[str]) -> Optional[str]:
    low = {c.lower(): c for c in cols}
    for c in cands:
        if c in low:
            return low[c]
    return None

# -----------------------------
# Entity registry & loaders
# -----------------------------
ENTITY_TABLES: Dict[str, List[str]] = {
    "team":        ["teams", "team"],
    "player":      ["players", "player"],
    "tournament":  ["tournaments", "tournament", "leagues", "league"],
    "sport":       ["sports", "sport"],
}

NAME_FIELDS = ["name", "title"]
TAGURL_FIELDS = ["tag_url", "tag", "url"]


def load_entities(con: sqlite3.Connection, etype: str) -> Tuple[str, List[Entity]]:
    table = None
    for cand in ENTITY_TABLES.get(etype, []):
        if table_exists(con, cand):
            table = cand
            break
    if not table:
        raise SystemExit(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ç–∏–ø–∞ '{etype}'. –û–∂–∏–¥–∞–ª–∏—Å—å: {ENTITY_TABLES.get(etype, [])}")

    cols = columns(con, table)
    id_col = pick(cols, ["id", f"{etype}_id"]) or cols[0]
    name_col = pick(cols, NAME_FIELDS) or id_col
    tag_col = pick(cols, TAGURL_FIELDS)
    if not tag_col:
        raise SystemExit(f"‚ùå –í —Ç–∞–±–ª–∏—Ü–µ '{table}' –Ω–µ—Ç tag_url (–æ–∂–∏–¥.: {TAGURL_FIELDS})")

    rows = con.execute(
        f"SELECT {id_col} AS id, {name_col} AS name, {tag_col} AS tag_url FROM {table} ORDER BY name"
    ).fetchall()
    return table, [Entity(int(r["id"]), str(r["name"]), str(r["tag_url"])) for r in rows]


def select_entity(entities: List[Entity], label: str) -> Entity:
    if not entities:
        raise SystemExit("‚ùå –°—É—â–Ω–æ—Å—Ç–µ–π –Ω–µ—Ç –≤ —Ç–∞–±–ª–∏—Ü–µ.")
    print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(entities)} {label}. –í–≤–µ–¥–∏—Ç–µ ID –∏–ª–∏ —Å—Ç—Ä–æ–∫—É –ø–æ–∏—Å–∫–∞, 'list' ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ 20.")
    while True:
        q = input("ID / –ø–æ–∏—Å–∫ / list: ").strip()
        if not q:
            continue
        if q.lower() == 'list':
            for e in entities[:20]:
                print(f"  {e.id:>6}  {e.name}")
            continue
        if q.isdigit():
            eid = int(q)
            for e in entities:
                if e.id == eid:
                    return e
            print("‚ö†Ô∏è –ù–µ—Ç —Ç–∞–∫–æ–π –∑–∞–ø–∏—Å–∏.")
            continue
        hits = [e for e in entities if q.lower() in e.name.lower()]
        if not hits:
            print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–Ω–∞—á–µ.")
            continue
        print(f"–ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(hits)}. –í—ã–±–µ—Ä–∏—Ç–µ ID (–ø–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ 20):")
        for e in hits[:20]:
            print(f"  {e.id:>6}  {e.name}")
        while True:
            s = input("–í–≤–µ–¥–∏—Ç–µ ID –∏–∑ —Å–ø–∏—Å–∫–∞: ").strip()
            if s.isdigit():
                eid = int(s)
                for e in hits:
                    if e.id == eid:
                        return e
                print("‚ö†Ô∏è ID –Ω–µ –∏–∑ —Å–ø–∏—Å–∫–∞.")
            else:
                print("‚ö†Ô∏è –í–≤–µ–¥–∏—Ç–µ —Ü–∏—Ñ—Ä–æ–≤–æ–π ID.")

# -----------------------------
# Period selection (with presets and robust input)
# -----------------------------

def select_period(tz: Optional[ZoneInfoType]) -> Tuple[datetime, datetime]:
    now = datetime.now(tz) if tz else datetime.now()
    while True:
        print("\n–í—ã–±–µ—Ä–∏—Ç–µ –ø–µ—Ä–∏–æ–¥:")
        print("  1) –ü–æ—Å–ª–µ–¥–Ω–∏–µ 6 —á–∞—Å–æ–≤")
        print("  2) –ü–æ—Å–ª–µ–¥–Ω–∏–µ 12 —á–∞—Å–æ–≤")
        print("  3) –ü–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞")
        print("  4) –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 –¥–Ω—è")
        print("  5) –ü–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π")
        print("  6) –°–µ–≥–æ–¥–Ω—è")
        print("  7) –í—á–µ—Ä–∞")
        print("  8) –ó–∞–¥–∞—Ç—å —è–≤–Ω–æ: FROM/TO")
        s = input("–í–∞—à –≤—ã–±–æ—Ä [1-8]: ").strip()
        if s == '1':
            return now - timedelta(hours=6), now
        if s == '2':
            return now - timedelta(hours=12), now
        if s == '3':
            return now - timedelta(days=1), now
        if s == '4':
            return now - timedelta(days=3), now
        if s == '5':
            return now - timedelta(days=7), now
        if s == '6':
            start = now.replace(hour=0, minute=0, second=0, microsecond=0)
            return start, now
        if s == '7':
            y = (now - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
            return y, y + timedelta(days=1)
        if s == '8':
            f = input("FROM (YYYY-MM-DD[ HH:MM], –ø—É—Å—Ç–æ ‚Äî –Ω–∞–∑–∞–¥): ").strip()
            if not f:
                continue
            t = input("TO   (YYYY-MM-DD[ HH:MM], –ø—É—Å—Ç–æ ‚Äî —Å–µ–π—á–∞—Å): ").strip()
            try:
                start = parse_dt_local(f, tz)
                end = parse_dt_local(t, tz) if t else (datetime.now(tz) if tz else datetime.now())
                if end <= start:
                    print("‚ö†Ô∏è TO –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–∑–∂–µ FROM."); continue
                return start, end
            except Exception as e:
                print("‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞—Ç—ã:", e); continue
        print("‚ö†Ô∏è –í–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ 1-8.")

# -----------------------------
# Tag helpers (smart lookup in tags)
# -----------------------------

def _norm_path(u: str) -> str:
    u = u.strip()
    m = re.sub(r"^https?://[^/]+", "", u, flags=re.I)
    m = m.rstrip('/')
    return m if m.startswith('/') else '/' + m


def find_tag_id_smart(con: sqlite3.Connection, tag_url: str, verbose: bool = False) -> int:
    if not table_exists(con, 'tags'):
        raise SystemExit("‚ùå –ù–µ—Ç —Ç–∞–±–ª–∏—Ü—ã 'tags'.")
    cols = columns(con, 'tags')
    url_col  = pick(cols, ['url','href'])
    id_col   = pick(cols, ['id','tag_id']) or cols[0]
    slug_col = pick(cols, ['slug'])
    name_col = pick(cols, ['name','title','label'])

    if verbose:
        print(f"üîñ tag_url: {tag_url}")

    # 1) —Ç–æ—á–Ω—ã–π url
    if url_col:
        row = con.execute(f"SELECT {id_col} FROM tags WHERE {url_col}=?", (tag_url,)).fetchone()
        if row:
            return int(row[0])

    # 2) –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –ø—É—Ç—å / —Ö–≤–æ—Å—Ç
    path = _norm_path(tag_url)
    tail = path.split('/')[-1]
    if url_col:
        row = con.execute(f"SELECT {id_col} FROM tags WHERE {url_col} IN (?, ?) LIMIT 1", (path, path.rstrip('/'))).fetchone()
        if row:
            return int(row[0])
        row = con.execute(f"SELECT {id_col} FROM tags WHERE {url_col} LIKE ? ORDER BY LENGTH({url_col}) DESC LIMIT 1", (f"%{tail}%",)).fetchone()
        if row:
            return int(row[0])

    # 3) slug / name
    if slug_col:
        row = con.execute(f"SELECT {id_col} FROM tags WHERE {slug_col}=? COLLATE NOCASE", (tail,)).fetchone()
        if row:
            return int(row[0])
    if name_col:
        row = con.execute(f"SELECT {id_col} FROM tags WHERE {name_col} LIKE ? COLLATE NOCASE ORDER BY LENGTH({name_col}) ASC LIMIT 1", (f"%{tail}%",)).fetchone()
        if row:
            return int(row[0])

    # 4) –ø–æ–∫–∞–∑–∞—Ç—å –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    candidates: List[Dict[str, Any]] = []
    if url_col:
        candidates += [dict(r) for r in con.execute(f"SELECT {id_col} AS id, {url_col} AS url FROM tags WHERE {url_col} LIKE ? LIMIT 20", (f"%{tail}%",))]
    if slug_col:
        candidates += [dict(r) for r in con.execute(f"SELECT {id_col} AS id, {slug_col} AS slug FROM tags WHERE {slug_col} LIKE ? LIMIT 20", (f"%{tail}%",))]
    if name_col and not candidates:
        candidates += [dict(r) for r in con.execute(f"SELECT {id_col} AS id, {name_col} AS name FROM tags WHERE {name_col} LIKE ? LIMIT 20", (f"%{tail}%",))]

    if candidates:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–±—Ä–∞—Ç—å –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ. –ö–∞–Ω–¥–∏–¥–∞—Ç—ã:")
        for r in candidates:
            print(" ", r)
        s = input("–í–≤–µ–¥–∏—Ç–µ –Ω—É–∂–Ω—ã–π tag_id –∏–∑ —Å–ø–∏—Å–∫–∞: ").strip()
        if s.isdigit():
            return int(s)

    raise SystemExit(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω tag –ø–æ url: {tag_url} (–ø—Ä–æ–±–æ–≤–∞–ª url/slug/name/LIKE)")

# -----------------------------
# Linkage ‚Üí news
# -----------------------------

def news_ids_by_tag(con: sqlite3.Connection, tag_id: int) -> List[int]:
    if not table_exists(con, 'news_article_tags'):
        raise SystemExit("‚ùå –ù–µ—Ç —Ç–∞–±–ª–∏—Ü—ã 'news_article_tags' (—Å–≤—è–∑–∏ –Ω–æ–≤–æ—Å—Ç—å‚Üî—Ç–µ–≥).")
    nat_cols = columns(con, 'news_article_tags')
    news_col = pick(nat_cols, ['news_id','article_id']) or nat_cols[0]
    tag_col  = pick(nat_cols, ['tag_id','label_id']) or nat_cols[-1]
    rows = con.execute(f"SELECT {news_col} AS nid FROM news_article_tags WHERE {tag_col}=?", (tag_id,)).fetchall()
    return [int(r['nid']) for r in rows]


def news_date_bounds(con: sqlite3.Connection, news_ids: List[int]) -> Optional[Tuple[str, str]]:
    if not news_ids:
        return None
    if not table_exists(con, 'news'):
        return None
    ncols = columns(con, 'news')
    id_col = pick(ncols, ['id','news_id']) or ncols[0]
    pub_col = pick(ncols, ['published_at','published','pub_date','date','datetime'])
    if not pub_col:
        return None
    placeholders = ",".join(["?"]*len(news_ids))
    row = con.execute(
        f"SELECT MIN({pub_col}) AS mn, MAX({pub_col}) AS mx FROM news WHERE {id_col} IN ({placeholders})",
        news_ids,
    ).fetchone()
    return (row["mn"], row["mx"]) if row and row["mn"] and row["mx"] else None


def fetch_news(con: sqlite3.Connection, news_ids: List[int], start: datetime, end: datetime, limit: Optional[int]) -> List[Article]:
    if not news_ids:
        return []
    if not table_exists(con, 'news'):
        raise SystemExit("‚ùå –ù–µ—Ç —Ç–∞–±–ª–∏—Ü—ã 'news'.")
    ncols = columns(con, 'news')
    id_col = pick(ncols, ['id','news_id']) or ncols[0]
    title_col = pick(ncols, ['title','headline','name']) or id_col
    url_col = pick(ncols, ['url','link']) or id_col
    pub_col = pick(ncols, ['published_at','published','pub_date','date','datetime'])
    source_col = pick(ncols, ['source','origin','site'])
    if not pub_col:
        raise SystemExit("‚ùå –í 'news' –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–æ–∂–∏–¥.: published_at/published/pub_date/date/datetime)")

    sT, eT = to_iso_T(start), to_iso_T(end)
    placeholders = ",".join(["?"]*len(news_ids))
    sql = (
        f"SELECT {id_col} AS id, {title_col} AS title, {url_col} AS url, {pub_col} AS published, "
        + (f"{source_col} AS source" if source_col else "NULL AS source") +
        f" FROM news WHERE {id_col} IN ({placeholders}) AND {pub_col}>=? AND {pub_col}<? ORDER BY {pub_col} DESC"
    )
    params: Tuple[Any, ...] = (*news_ids, sT, eT)
    if limit:
        sql += " LIMIT ?"
        params += (limit,)

    out: List[Article] = []
    for r in con.execute(sql, params):
        out.append(Article(int(r['id']), str(r['title']), str(r['url']), str(r['published']), r['source']))
    return out

# -----------------------------
# Fallback parse (optional, with headers to avoid 403)
# -----------------------------
NEWS_ITEM_RE = re.compile(r'<div class="news-item__time">(?P<time>\d{2}:\d{2})</div>.*?<a href="(?P<href>/[^"]+)"[^>]*class="news-item__title[^"]*">(?P<title>[^<]+)</a>', re.S|re.I)
DATE_HEAD_RE = re.compile(r'<div class="news-items__head">(?P<date>\d{1,2} [–∞-—è–ê-–Ø]+ \d{4})</div>')
MONTHS_RU = {'—è–Ω–≤–∞—Ä—è':1,'—Ñ–µ–≤—Ä–∞–ª—è':2,'–º–∞—Ä—Ç–∞':3,'–∞–ø—Ä–µ–ª—è':4,'–º–∞—è':5,'–∏—é–Ω—è':6,'–∏—é–ª—è':7,'–∞–≤–≥—É—Å—Ç–∞':8,'—Å–µ–Ω—Ç—è–±—Ä—è':9,'–æ–∫—Ç—è–±—Ä—è':10,'–Ω–æ—è–±—Ä—è':11,'–¥–µ–∫–∞–±—Ä—è':12}


def parse_tag_page(tag_url: str, base: str = "https://www.championat.com") -> List[Article]:
    if not requests:
        print("‚ö†Ô∏è requests –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω ‚Äî fallback-–ø–∞—Ä—Å–µ—Ä –æ—Ç–∫–ª—é—á—ë–Ω")
        return []
    sess = requests.Session()
    sess.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "ru-RU,ru;q=0.9",
        "Referer": "https://www.championat.com/",
        "Connection": "keep-alive",
    })
    try:
        r = sess.get(tag_url, timeout=20)
        if r.status_code == 403:
            import time
            from random import randint
            time.sleep(1 + randint(0, 2))
            r = sess.get(tag_url + f"?_ts={int(time.time())}", timeout=20)
        r.raise_for_status()
    except Exception as e:
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ç–µ–≥–∞:", e)
        return []

    html = r.text
    mdate = DATE_HEAD_RE.search(html)
    date_obj = None
    if mdate:
        try:
            dd, mm_rus, yyyy = mdate.group('date').split()
            date_obj = datetime(int(yyyy), MONTHS_RU[mm_rus.lower()], int(dd))
        except Exception:
            date_obj = None

    arts: List[Article] = []
    for m in NEWS_ITEM_RE.finditer(html):
        hh, mm = m.group('time').split(':')
        dt = datetime(date_obj.year, date_obj.month, date_obj.day, int(hh), int(mm)) if date_obj else None
        title = m.group('title').strip()
        href = m.group('href')
        full_url = href if href.startswith('http') else base.rstrip('/') + href
        iso = dt.isoformat(timespec='seconds') if dt else ''
        arts.append(Article(-1, title, full_url, iso, 'championat'))
    return arts

# -----------------------------
# Export helpers
# -----------------------------

def export_results(arts: List[Article], kind: Optional[str], path: Optional[str]) -> None:
    if not arts or not kind or not path:
        return
    os.makedirs(os.path.dirname(path) or '.', exist_ok=True)
    if kind.lower() == 'csv':
        with open(path, 'w', newline='', encoding='utf-8') as f:
            w = csv.DictWriter(f, fieldnames=['id','title','url','published_at','source'])
            w.writeheader()
            for a in arts:
                w.writerow(asdict(a))
        print(f"üíæ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤ CSV: {path}")
    elif kind.lower() == 'json':
        with open(path, 'w', encoding='utf-8') as f:
            json.dump([asdict(a) for a in arts], f, ensure_ascii=False, indent=2)
        print(f"üíæ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤ JSON: {path}")

# -----------------------------
# CLI
# -----------------------------

def main() -> None:
    import argparse
    ap = argparse.ArgumentParser(description="–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤: sport/tournament/team/player ‚Üí –ø–µ—Ä–∏–æ–¥ ‚Üí –Ω–æ–≤–æ—Å—Ç–∏")
    ap.add_argument("--db", default=os.environ.get("DB_PATH", "./database/prosport.db"))
    ap.add_argument("-v","--verbose", action="store_true")
    ap.add_argument("--limit", type=int, default=100)
    ap.add_argument("--export", choices=['csv','json'])
    ap.add_argument("--export-path")
    args = ap.parse_args()

    tz = get_tz()

    con = connect_db(args.db, verbose=args.verbose)
    try:
        # 1) –í—ã–±–æ—Ä —Ç–∏–ø–∞ —Å—É—â–Ω–æ—Å—Ç–∏
        print("–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø —Å—É—â–Ω–æ—Å—Ç–∏:")
        print("  1) sport")
        print("  2) tournament")
        print("  3) team")
        print("  4) player")
        map_idx = {'1':'sport','2':'tournament','3':'team','4':'player'}
        etype = None
        while etype is None:
            s = input("[1-4]: ").strip()
            etype = map_idx.get(s)
        table, entities = load_entities(con, etype)
        ent = select_entity(entities, label=f"–∑–∞–ø–∏—Å–µ–π –≤ '{table}'")
        print(f"\n‚úÖ –í—ã–±—Ä–∞–Ω–æ: [{ent.id}] {ent.name}")

        # 2) –°–≤—è–∑–∏: tag ‚Üí news_ids
        tag_id = find_tag_id_smart(con, ent.tag_url, verbose=args.verbose)
        news_ids = news_ids_by_tag(con, tag_id)
        if args.verbose:
            print("üè∑Ô∏è tag_id:", tag_id)
            print("üß© news_ids count:", len(news_ids))
        bounds = news_date_bounds(con, news_ids)
        if bounds:
            print(f"üóìÔ∏è  –í –±–∞–∑–µ –¥–ª—è —ç—Ç–æ–≥–æ —Ç–µ–≥–∞: –æ—Ç {bounds[0]} –¥–æ {bounds[1]}")

        # 3) –ü–µ—Ä–∏–æ–¥
        start, end = select_period(tz)
        print(f"‚è±Ô∏è  –î–∏–∞–ø–∞–∑–æ–Ω: [{start:%Y-%m-%d %H:%M}, {end:%Y-%m-%d %H:%M})")

        # 4) –í—ã–±–æ—Ä–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
        arts = fetch_news(con, news_ids, start, end, args.limit)
    finally:
        con.close()

    if arts:
        print(f"\nüîé –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(arts)}\n")
        for a in arts:
            print(f"[{a.published_at}] ‚Äî {a.title}\n{a.url}\n")
        export_results(arts, args.export, args.export_path)
        return

    print("\n–ù–µ—Ç —Å—Ç–∞—Ç–µ–π –≤ –∑–∞–¥–∞–Ω–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ.")
    ans = input("–ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å fallback‚Äë–ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ç–µ–≥–∞? [y/N]: ").strip().lower()
    if ans == 'y':
        if requests is None:
            print("‚ö†Ô∏è –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install requests")
            return
        try:
            tmp = parse_tag_page(ent.tag_url)
            if not tmp:
                print("(–Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ)")
                return
            sT, eT = to_iso_T(start), to_iso_T(end)
            out = [a for a in tmp if a.published_at and (sT <= a.published_at < eT)]
            print(f"üìù –Ω–∞–π–¥–µ–Ω–æ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(tmp)}, –≤ –æ–∫–Ω–µ: {len(out)}\n")
            for a in out:
                print(f"[{a.published_at}] ‚Äî {a.title}\n{a.url}\n")
            export_results(out, args.export, args.export_path)
        except Exception as e:
            print("‚ùå fallback-–ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è:", e)


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
fetch_entity_news_on_demand.py ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —Å—É—â–Ω–æ—Å—Ç–∏ –∑–∞ —Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ
(–∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –ø–æ–¥ —Å—Ö–µ–º—É –ë–î: —Å—Ç–∞—Ç—å–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ `news`, —Å–≤—è–∑–∏ ‚Äî `news_articles_tags`).

‚öôÔ∏è –ß—Ç–æ —É–º–µ–µ—Ç
- –û–∫–Ω–æ: —Å—Ç—Ä–æ–≥–æ [start, end) ‚Äî –ø—Ä–∏ --days 1 —ç—Ç–æ —Ä–æ–≤–Ω–æ 24 —á–∞—Å–∞ –¥–æ —Ç–µ–∫—É—â–µ–≥–æ –º–æ–º–µ–Ω—Ç–∞
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 2 –≤–∞—Ä–∏–∞–Ω—Ç–∞ —Å–≤—è–∑–µ–π:
  A) `news_articles_tags` —Å–æ–¥–µ—Ä–∂–∏—Ç (news_id, entity_type, entity_id)
  B) `news_articles_tags` —Å–æ–¥–µ—Ä–∂–∏—Ç (news_id, tag_id), —Ç–æ–≥–¥–∞ –∏—â–µ–º —Ç–∞–±–ª–∏—Ü—É —Ç–µ–≥–æ–≤
     (–Ω–∞–ø—Ä–∏–º–µ—Ä `tags`) –∏ —Ç–∞–±–ª–∏—Ü—É –∞–ª–∏–∞—Å–æ–≤ (`entity_aliases`) –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
     tag.name ‚Üí (entity_type, entity_id)
- –ê–≤—Ç–æ–±—ç–∫–æ—Ñ–∏–ª–ª –ø–æ –∂–µ–ª–∞–Ω–∏—é (`--backfill-run`) –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ (`--debug-scan`)
- –ê–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç –ø–æ–ª–µ–π –≤ `news`: id, title, url, published_at (–≥–∏–±–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è)

–ó–∞–ø—É—Å–∫ (–ø—Ä–∏–º–µ—Ä):
  python -m scripts.fetch_entity_news_on_demand --entity team --id 912 --days 1 -v \
      --db F:/projects/Projects/sport-news-bot/database/prosport.db --debug-scan
"""
from __future__ import annotations

import argparse
import os
import sqlite3
import subprocess
import sys
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple

# -----------------------------
# TZ shim
# -----------------------------
try:
    from zoneinfo import ZoneInfo  # py>=3.9
except Exception:  # pragma: no cover
    try:
        from backports.zoneinfo import ZoneInfo  # type: ignore
    except Exception:  # pragma: no cover
        ZoneInfo = None  # type: ignore

SUPPORTED_ENTITY_TYPES = {"sport", "tournament", "team", "player"}


@dataclass
class Article:
    id: int
    title: str
    url: str
    published_at: datetime
    source: Optional[str]


# -----------------------------
# Helpers: TZ & datetime
# -----------------------------

def get_tz() -> Optional["ZoneInfo"]:
    tz_name = os.environ.get("APP_TZ") or os.environ.get("TZ") or "UTC"
    try:
        return ZoneInfo(tz_name) if ZoneInfo else None
    except Exception:
        return None


def parse_dt_local(s: str, tz: Optional["ZoneInfo"]) -> datetime:
    s = s.strip()
    fmt = "%Y-%m-%d %H:%M" if " " in s else "%Y-%m-%d"
    dt = datetime.strptime(s, fmt)
    return dt.replace(tzinfo=tz) if tz else dt


def _to_sqlite_iso(dt: datetime) -> str:
    """Convert dt to naive UTC ISO for SQLite (space separator)."""
    if dt.tzinfo is not None and ZoneInfo:
        dt = dt.astimezone(ZoneInfo("UTC")).replace(tzinfo=None)
    return dt.isoformat(timespec="seconds").replace("T", " ")


def parse_sqlite_dt(s: str, tz: Optional["ZoneInfo"]) -> datetime:
    s = s.replace("T", " ")
    # –ø–æ–¥–¥–µ—Ä–∂–∏–º "YYYY-MM-DD HH:MM" –∏ "YYYY-MM-DD HH:MM:SS"
    for fmt in ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M"):
        try:
            dt = datetime.strptime(s, fmt)
            return dt.replace(tzinfo=tz) if tz else dt
        except ValueError:
            continue
    # –∫–∞–∫ fallback ‚Äî —Ç–æ–ª—å–∫–æ –¥–∞—Ç–∞
    dt = datetime.strptime(s.split(" ")[0], "%Y-%m-%d")
    return dt.replace(tzinfo=tz) if tz else dt


# -----------------------------
# Helpers: DB path (Windows-safe)
# -----------------------------

def resolve_db_path(db_arg: str) -> str:
    raw = db_arg.strip()
    if raw.lower().startswith("sqlite:///"):
        raw = raw[10:]
    raw = os.path.expandvars(os.path.expanduser(raw))
    raw = os.path.normpath(raw)
    abs_path = os.path.abspath(raw)
    parent = os.path.dirname(abs_path)
    if parent and not os.path.exists(parent):
        os.makedirs(parent, exist_ok=True)
    return abs_path


def connect_db(db_path: str, verbose: bool = False) -> sqlite3.Connection:
    resolved = resolve_db_path(db_path)
    if verbose:
        print(f"üóÑÔ∏è  DB path resolved to: {resolved}")
    try:
        conn = sqlite3.connect(resolved)
    except sqlite3.OperationalError as e:
        raise SystemExit(
            f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å SQLite —Ñ–∞–π–ª: {e}\n–ó–∞–ø—Ä–æ—à–µ–Ω–æ: {db_path}\n–†–µ–∑–æ–ª–≤–Ω—É—Ç–æ: {resolved}\n"
        )
    conn.row_factory = sqlite3.Row
    conn.execute("PRAGMA foreign_keys = ON;")
    return conn


# -----------------------------
# Schema helpers (auto-detect)
# -----------------------------

def table_exists(conn: sqlite3.Connection, name: str) -> bool:
    cur = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name=?", (name,))
    return cur.fetchone() is not None


def get_columns(conn: sqlite3.Connection, table: str) -> List[str]:
    cur = conn.execute(f"PRAGMA table_info({table})")
    return [r[1] for r in cur.fetchall()]


def pick_first(cols: List[str], candidates: List[str]) -> Optional[str]:
    low = {c.lower(): c for c in cols}
    for name in candidates:
        if name in low:
            return low[name]
    return None


def detect_news_columns(conn: sqlite3.Connection) -> Dict[str, Optional[str]]:
    cols = get_columns(conn, "news")
    # id/title/url/published/source ‚Äî –≥–∏–±–∫–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è
    id_col = pick_first(cols, ["id", "news_id"]) or cols[0]
    title_col = pick_first(cols, ["title", "headline", "name"]) or None
    url_col = pick_first(cols, ["url", "link", "permalink"]) or None
    published_col = pick_first(cols, [
        "published_at", "published", "pub_date", "date", "created_at", "datetime", "time"
    ]) or None
    source_col = pick_first(cols, ["source", "origin", "site"]) or None
    return {
        "id": id_col,
        "title": title_col,
        "url": url_col,
        "published": published_col,
        "source": source_col,
    }


# -----------------------------
# Core query (news + news_articles_tags)
# -----------------------------

def fetch_articles(
    conn: sqlite3.Connection,
    entity_type: str,
    entity_id: int,
    start_dt: datetime,
    end_dt: datetime,
    limit: Optional[int] = None,
) -> List[Article]:
    if not table_exists(conn, "news"):
        raise SystemExit("‚ùå –í –ë–î –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–∞–±–ª–∏—Ü–∞ 'news'.")
    if not table_exists(conn, "news_articles_tags"):
        # –Ω–µ—Ç —Å–≤—è–∑–µ–π ‚Äî –≤–µ—Ä–Ω—É—Ç—å –ø—É—Å—Ç–æ (–∏ –ø–æ–¥—Å–∫–∞–∑–∞—Ç—å –±—ç–∫–æ—Ñ–∏–ª–ª/–º–∏–≥—Ä–∞—Ü–∏—é)
        return []

    N = detect_news_columns(conn)
    id_col = N["id"]
    title_col = N["title"] or N["id"]
    url_col = N["url"] or N["id"]
    pub_col = N["published"]
    src_col = N["source"]

    if not pub_col:
        raise SystemExit("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–æ–Ω–∫—É –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ 'news'. –î–æ–±–∞–≤—å—Ç–µ –æ–¥–Ω—É –∏–∑: published_at/published/pub_date/date/created_at/datetime/time")

    start_iso = _to_sqlite_iso(start_dt)
    end_iso = _to_sqlite_iso(end_dt)

    # –ü–æ–π–º—ë–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É news_articles_tags
    nat_cols = get_columns(conn, "news_articles_tags")
    has_entity_cols = set(c.lower() for c in nat_cols) >= {"news_id", "entity_type", "entity_id"}

    results: List[Article] = []

    if has_entity_cols:
        # –í–∞—Ä–∏–∞–Ω—Ç A: –ø—Ä—è–º—ã–µ entity_type/entity_id
        sql = (
            f"SELECT a.{id_col} AS id, a.{title_col} AS title, a.{url_col} AS url, a.{pub_col} AS published, "
            + (f"a.{src_col} AS source " if src_col else "NULL AS source ") +
            "FROM news a "
            "JOIN news_articles_tags nat ON nat.news_id = a." + id_col + " "
            "WHERE nat.entity_type = ? AND nat.entity_id = ? "
            f"AND a.{pub_col} >= ? AND a.{pub_col} < ? "
            f"ORDER BY a.{pub_col} DESC"
        )
        params: Tuple = (entity_type, entity_id, start_iso, end_iso)
        if limit:
            sql += " LIMIT ?"
            params += (limit,)
        cur = conn.execute(sql, params)
        for r in cur.fetchall():
            results.append(
                Article(
                    id=r["id"],
                    title=str(r["title"]),
                    url=str(r["url"]),
                    published_at=parse_sqlite_dt(str(r["published"]), tz=None),
                    source=r["source"],
                )
            )
        return results

    # –í–∞—Ä–∏–∞–Ω—Ç B: —á–µ—Ä–µ–∑ tags ‚Üí entity_aliases
    # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ç–∞–±–ª–∏—Ü—É —Å —Ç–µ–≥–∞–º–∏
    tag_table = None
    for cand in ("tags", "news_tags", "article_tags", "labels"):
        if table_exists(conn, cand):
            tag_table = cand
            break
    if not tag_table:
        # –Ω–µ—Ç —Å–ø–æ—Å–æ–±–∞ —Å–≤—è–∑–∞—Ç—å —Ç–µ–≥–∏ ‚Üí —Å—É—â–Ω–æ—Å—Ç–∏
        return []

    tag_cols = get_columns(conn, tag_table)
    tag_id = pick_first(tag_cols, ["id", "tag_id"]) or tag_cols[0]
    tag_name = pick_first(tag_cols, ["name", "title", "label"]) or tag_cols[0]

    if not table_exists(conn, "entity_aliases"):
        # –±–µ–∑ –∞–ª–∏–∞—Å–æ–≤ –Ω–µ–ª—å–∑—è –º–∞–ø–ø–∏—Ç—å —Ç–µ–≥–∏ –Ω–∞ —Å—É—â–Ω–æ—Å—Ç–∏
        return []

    # entity_aliases: alias (lowercase), entity_type, entity_id
    # –¥–æ–ø—É—Å—Ç–∏–º—ã –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–∑–≤–∞–Ω–∏–π
    ea_cols = get_columns(conn, "entity_aliases")
    ea_alias = pick_first(ea_cols, ["alias", "name"]) or ea_cols[0]
    ea_type = pick_first(ea_cols, ["entity_type", "type"]) or ea_cols[0]
    ea_id = pick_first(ea_cols, ["entity_id", "id"]) or ea_cols[0]

    sql = (
        f"SELECT a.{id_col} AS id, a.{title_col} AS title, a.{url_col} AS url, a.{pub_col} AS published, "
        + (f"a.{src_col} AS source " if src_col else "NULL AS source ") +
        "FROM news a "
        "JOIN news_articles_tags nat ON nat.news_id = a." + id_col + " "
        f"JOIN {tag_table} t ON t.{tag_id} = nat.tag_id "
        f"JOIN entity_aliases ea ON ea.{ea_alias} = LOWER(t.{tag_name}) "
        f"WHERE ea.{ea_type} = ? AND ea.{ea_id} = ? "
        f"AND a.{pub_col} >= ? AND a.{pub_col} < ? "
        f"ORDER BY a.{pub_col} DESC"
    )
    params = (entity_type, entity_id, start_iso, end_iso)
    if limit:
        sql += " LIMIT ?"
        params += (limit,)

    cur = conn.execute(sql, params)
    for r in cur.fetchall():
        results.append(
            Article(
                id=r["id"],
                title=str(r["title"]),
                url=str(r["url"]),
                published_at=parse_sqlite_dt(str(r["published"]), tz=None),
                source=r["source"],
            )
        )

    return results


# -----------------------------
# Diagnostics & Backfill
# -----------------------------

def debug_counts(conn: sqlite3.Connection, entity: str, eid: int, start_dt: datetime, end_dt: datetime) -> None:
    s_iso = _to_sqlite_iso(start_dt)
    e_iso = _to_sqlite_iso(end_dt)

    have_news = table_exists(conn, "news")
    have_nat = table_exists(conn, "news_articles_tags")
    print(f"üß™ debug: tables news={have_news}, news_articles_tags={have_nat}, window=[{s_iso}, {e_iso})")

    if not (have_news and have_nat):
        return

    nat_cols = [c.lower() for c in get_columns(conn, "news_articles_tags")]
    direct = linked = None

    # –æ–±–Ω–∞—Ä—É–∂–∏–º –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ published
    N = detect_news_columns(conn)
    pub_col = N["published"]
    if not pub_col:
        print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ news")
        return

    if {"news_id", "entity_type", "entity_id"}.issubset(set(nat_cols)):
        cur = conn.execute(
            f"SELECT COUNT(*) FROM news a JOIN news_articles_tags nat ON nat.news_id=a.{N['id']} "
            f"WHERE nat.entity_type=? AND nat.entity_id=? AND a.{pub_col}>=? AND a.{pub_col}<?",
            (entity, eid, s_iso, e_iso),
        )
        direct = cur.fetchone()[0]
    if "tag_id" in nat_cols and table_exists(conn, "entity_aliases"):
        # –Ω–∞–π–¥—ë–º —Ç–∞–±–ª–∏—Ü—É —Ç–µ–≥–æ–≤
        tname = None
        for cand in ("tags", "news_tags", "article_tags", "labels"):
            if table_exists(conn, cand):
                tname = cand
                break
        if tname:
            tcols = get_columns(conn, tname)
            t_id = pick_first(tcols, ["id", "tag_id"]) or tcols[0]
            t_name = pick_first(tcols, ["name", "title", "label"]) or tcols[0]
            ea_cols = get_columns(conn, "entity_aliases")
            ea_alias = pick_first(ea_cols, ["alias", "name"]) or ea_cols[0]
            ea_type = pick_first(ea_cols, ["entity_type", "type"]) or ea_cols[0]
            ea_id = pick_first(ea_cols, ["entity_id", "id"]) or ea_cols[0]
            cur = conn.execute(
                f"SELECT COUNT(*) FROM news a "
                f"JOIN news_articles_tags nat ON nat.news_id=a.{N['id']} "
                f"JOIN {tname} t ON t.{t_id} = nat.tag_id "
                f"JOIN entity_aliases ea ON ea.{ea_alias} = LOWER(t.{t_name}) "
                f"WHERE ea.{ea_type}=? AND ea.{ea_id}=? AND a.{pub_col}>=? AND a.{pub_col}<?",
                (entity, eid, s_iso, e_iso),
            )
            linked = cur.fetchone()[0]

    print(f"   ‚Ä¢ direct={direct if direct is not None else '(–Ω–µ—Ç –ø—Ä—è–º—ã—Ö entity_* –≤ news_articles_tags)'}")
    print(f"   ‚Ä¢ via_alias={linked if linked is not None else '(–Ω–µ—Ç tag_id/alias-—Å–≤—è–∑–∫–∏)'}")


def suggest_backfill(entity_type: str, entity_id: int, start_dt: datetime, end_dt: datetime) -> None:
    print("\n‚ö†Ô∏è –í —É–∫–∞–∑–∞–Ω–Ω–æ–º –æ–∫–Ω–µ –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
    print("–í—ã –º–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å –±—ç–∫–æ—Ñ–∏–ª–ª –ø–∞—Ä—Å–µ—Ä–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä:")
    print(
        f"  python -m parsers.championat.fetch --entity {entity_type} --id {entity_id} "
        f"--from \"{start_dt:%Y-%m-%d %H:%M}\" --to \"{end_dt:%Y-%m-%d %H:%M}\" -v"
    )


def run_backfill(entity: str, eid: int, start_dt: datetime, end_dt: datetime, verbose: bool = False) -> int:
    cmd = [
        sys.executable,
        "-m",
        "parsers.championat.fetch",
        "--entity",
        entity,
        "--id",
        str(eid),
        "--from",
        start_dt.strftime("%Y-%m-%d %H:%M"),
        "--to",
        end_dt.strftime("%Y-%m-%d %H:%M"),
        "-v",
    ]
    if verbose:
        print("üß© backfill cmd:", " ".join(cmd))
    try:
        return subprocess.call(cmd)
    except FileNotFoundError:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –º–æ–¥—É–ª—å parsers.championat.fetch ‚Äî –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞.")
        return 127


# -----------------------------
# CLI
# -----------------------------

def main():
    tz = get_tz()

    parser = argparse.ArgumentParser(
        description="–ü–æ–∫–∞–∑–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —Å—É—â–Ω–æ—Å—Ç–∏ –∑–∞ –∑–∞–¥–∞–Ω–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω (—Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ).",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument("--entity", required=True, choices=sorted(SUPPORTED_ENTITY_TYPES), help="–¢–∏–ø —Å—É—â–Ω–æ—Å—Ç–∏")
    parser.add_argument("--id", required=True, type=int, help="ID —Å—É—â–Ω–æ—Å—Ç–∏")
    parser.add_argument("--days", type=int, default=1, help="–†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –≤ —Å—É—Ç–∫–∞—Ö, –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω—ã --from/--to")
    parser.add_argument("--from", dest="from_dt", type=str, help="–ù–∞—á–∞–ª–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ (–ª–æ–∫–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è) YYYY-MM-DD[ HH:MM]")
    parser.add_argument("--to", dest="to_dt", type=str, help="–ö–æ–Ω–µ—Ü –¥–∏–∞–ø–∞–∑–æ–Ω–∞ (–ª–æ–∫–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è) YYYY-MM-DD[ HH:MM]")
    parser.add_argument("--limit", type=int, default=100, help="–û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª-–≤–æ –≤—ã–≤–æ–¥–∏–º—ã—Ö —Å—Ç–∞—Ç–µ–π")
    parser.add_argument("-v", "--verbose", action="store_true", help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥")
    parser.add_argument("--db", default=os.environ.get("DB_PATH", "./database/prosport.db"), help="–ü—É—Ç—å –∫ SQLite –±–∞–∑–µ")
    parser.add_argument("--backfill", action="store_true", help="–ü–æ–¥—Å–∫–∞–∑–∞—Ç—å –∫–æ–º–∞–Ω–¥—É –±—ç–∫–æ—Ñ–∏–ª–ª–∞, –µ—Å–ª–∏ –ø—É—Å—Ç–æ")
    parser.add_argument("--backfill-run", action="store_true", help="–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –±—ç–∫–æ—Ñ–∏–ª–ª, –µ—Å–ª–∏ –ø—É—Å—Ç–æ")
    parser.add_argument("--debug-scan", action="store_true", help="–ü–æ–∫–∞–∑–∞—Ç—å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É –ø–æ —Å–≤—è–∑—è–º –∏ –æ–∫–Ω—É")

    args = parser.parse_args()

    now = datetime.now(tz) if tz else datetime.now()

    if args.from_dt:
        start_dt = parse_dt_local(args.from_dt, tz)
        end_dt = parse_dt_local(args.to_dt, tz) if args.to_dt else now
    else:
        # –°–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ [now - days, now)
        start_dt = now - timedelta(days=args.days)
        end_dt = parse_dt_local(args.to_dt, tz) if args.to_dt else now

    if end_dt <= start_dt:
        parser.error("–ö–æ–Ω–µ—Ü –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±–æ–ª—å—à–µ –Ω–∞—á–∞–ª–∞.")

    if args.verbose:
        tz_str = getattr(tz, "key", "naive") if tz else "naive"
        print("‚öôÔ∏è  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞:")
        print(f"   ‚Ä¢ entity: {args.entity}")
        print(f"   ‚Ä¢ id:     {args.id}")
        print(f"   ‚Ä¢ window: [{start_dt:%Y-%m-%d %H:%M}, {end_dt:%Y-%m-%d %H:%M})")
        print(f"   ‚Ä¢ db:     {args.db}")
        print(f"   ‚Ä¢ tz:     {tz_str}\n")

    conn = connect_db(args.db, verbose=args.verbose)
    try:
        if args.debug_scan:
            debug_counts(conn, args.entity, args.id, start_dt, end_dt)
        articles = fetch_articles(
            conn=conn,
            entity_type=args.entity,
            entity_id=args.id,
            start_dt=start_dt,
            end_dt=end_dt,
            limit=args.limit,
        )
    finally:
        conn.close()

    if args.verbose:
        print(f"üîé –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}\n")

    if not articles:
        print("–ù–µ—Ç —Å—Ç–∞—Ç–µ–π –≤ –∑–∞–¥–∞–Ω–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ.")
        if args.backfill_run:
            rc = run_backfill(args.entity, args.id, start_dt, end_dt, verbose=args.verbose)
            print(f"üîÅ backfill exit code: {rc}")
            return
        if args.backfill:
            suggest_backfill(args.entity, args.id, start_dt, end_dt)
        return

    for a in articles:
        when = a.published_at.strftime("%Y-%m-%d %H:%M")
        src = f" ¬∑ {a.source}" if a.source else ""
        print(f"[{when}]{src} ‚Äî {a.title}\n{a.url}\n")


if __name__ == "__main__":
    main()

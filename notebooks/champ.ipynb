{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de804a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-01 20:15:14,563 - INFO - üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π championat.com\n",
      "2025-09-01 20:15:14,564 - INFO - üìÑ –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã 1\n",
      "2025-09-01 20:15:14,565 - INFO - –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: https://www.championat.com/news/1.html\n",
      "2025-09-01 20:15:23,007 - INFO - ‚úÖ –ù–∞–π–¥–µ–Ω—ã –Ω–æ–≤–æ—Å—Ç–∏ —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º: .page-content .news-item\n",
      "2025-09-01 20:15:23,008 - INFO - –ù–∞–π–¥–µ–Ω–æ 5 –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ\n",
      "2025-09-01 20:15:23,077 - INFO - [1/5] –ù–∞–π–¥–µ–Ω–∞: –ì–∞—Ä–∞–Ω–∏–Ω: ¬´–ë–∞–ª—Ç–∏–∫–∞¬ª –≤—ã–¥–∞–ª–∞ –æ–±–∞–ª–¥–µ–Ω–Ω—ã–π —Å—Ç–∞—Ä—Ç, –æ–Ω–∏ –Ω–µ...\n",
      "2025-09-01 20:15:23,127 - INFO - [2/5] –ù–∞–π–¥–µ–Ω–∞: –ö—Ä–∏—Å—Ç–∞–ø—Å –ü–æ—Ä–∑–∏–Ω–≥–∏—Å, –ß–ï-2025: —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤ –º–∞—Ç—á–µ –ü–æ...\n",
      "2025-09-01 20:15:23,176 - INFO - [3/5] –ù–∞–π–¥–µ–Ω–∞: –¢—Ä–µ–Ω–µ—Ä ¬´–õ–µ–π–∫–µ—Ä—Å¬ª —Ä–∞—Å—Å–∫–∞–∑–∞–ª, –∫–∞–∫ –∏–º–µ–Ω–Ω–æ –õ–µ–±—Ä–æ–Ω –î–∂–µ–π...\n",
      "2025-09-01 20:15:23,232 - INFO - [4/5] –ù–∞–π–¥–µ–Ω–∞: –í –ö–∏—Ç–∞–µ –¥–µ—Ç–µ–π –∑–∞—Å—Ç–∞–≤–ª—è—é—Ç –∏–≥—Ä–∞—Ç—å –≤ –∏–≥—Ä—ã –ø–æ 15 —á–∞—Å–æ–≤...\n",
      "2025-09-01 20:15:23,276 - INFO - [5/5] –ù–∞–π–¥–µ–Ω–∞: 21 –æ—á–∫–æ –ü–æ—Ä–∑–∏–Ω–≥–∏—Å–∞ –ø–æ–º–æ–≥–ª–æ —Å–±–æ—Ä–Ω–æ–π –õ–∞—Ç–≤–∏–∏ –ø–æ–±–µ–¥–∏—Ç—å...\n",
      "2025-09-01 20:15:23,279 - INFO - –°–æ–±—Ä–∞–Ω–æ 5 —Å—Å—ã–ª–æ–∫ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞\n",
      "2025-09-01 20:15:23,282 - INFO - [1/5] –ü–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª–∏: –ì–∞—Ä–∞–Ω–∏–Ω: ¬´–ë–∞–ª—Ç–∏–∫–∞¬ª –≤—ã–¥–∞–ª–∞ –æ–±–∞–ª–¥–µ–Ω–Ω—ã–π —Å—Ç–∞—Ä—Ç, –æ–Ω–∏ –Ω–µ...\n",
      "2025-09-01 20:15:28,907 - INFO - ‚úÖ –¢–µ–∫—Å—Ç –Ω–∞–π–¥–µ–Ω —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º: article p\n",
      "2025-09-01 20:15:29,027 - WARNING - –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É: 1 –°–ï–ù–¢–Ø–ë–†–Ø 2025, 20:10 –ú–°–ö\n",
      "2025-09-01 20:15:29,242 - INFO - ‚úÖ –ù–∞–π–¥–µ–Ω–æ 5 —Ç–µ–≥–æ–≤\n",
      "2025-09-01 20:15:29,299 - INFO - ‚úÖ –ù–∞–π–¥–µ–Ω–æ 1 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
      "2025-09-01 20:15:29,383 - INFO - ‚úÖ –ù–æ–≤–æ—Å—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–∞: –ì–∞—Ä–∞–Ω–∏–Ω: ¬´–ë–∞–ª—Ç–∏–∫–∞¬ª –≤—ã–¥–∞–ª–∞ –æ–±–∞–ª–¥–µ–Ω–Ω—ã–π —Å—Ç–∞—Ä—Ç, –æ–Ω–∏ –Ω–µ...\n",
      "2025-09-01 20:15:29,384 - INFO - üìä –í—Å–µ–≥–æ —Å–ø–∞—Ä—à–µ–Ω–æ: 1 –Ω–æ–≤–æ—Å—Ç–µ–π\n",
      "2025-09-01 20:15:31,386 - ERROR - –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –Ω–æ–≤–æ—Å—Ç–∏ 2: 'url'\n",
      "2025-09-01 20:15:31,388 - ERROR - –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –Ω–æ–≤–æ—Å—Ç–∏ 3: 'url'\n",
      "2025-09-01 20:15:31,389 - ERROR - –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –Ω–æ–≤–æ—Å—Ç–∏ 4: 'url'\n",
      "2025-09-01 20:15:31,390 - ERROR - –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –Ω–æ–≤–æ—Å—Ç–∏ 5: 'url'\n",
      "2025-09-01 20:15:31,391 - INFO - –ó–∞–≤–µ—Ä—à–µ–Ω–æ. –°–ø–∞—Ä—à–µ–Ω–æ: 1, –ü—Ä–æ–ø—É—â–µ–Ω–æ: 0\n",
      "2025-09-01 20:15:31,393 - ERROR - –û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: 'url'\n",
      "2025-09-01 20:15:31,396 - INFO - üéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω! –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: 1\n",
      "2025-09-01 20:15:31,461 - INFO - üìÅ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ parsed_news.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä –°–í–û–î–ö–ê –ü–ê–†–°–ò–ù–ì–ê: 1 –Ω–æ–≤–æ—Å—Ç–µ–π\n",
      "================================================================================\n",
      "\n",
      "1. –ì–∞—Ä–∞–Ω–∏–Ω: ¬´–ë–∞–ª—Ç–∏–∫–∞¬ª –≤—ã–¥–∞–ª–∞ –æ–±–∞–ª–¥–µ–Ω–Ω—ã–π —Å—Ç–∞—Ä—Ç, –æ–Ω–∏ –Ω–µ –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞—é—Ç—Å—è –¥–∞–∂–µ –ø–æ–¥ –≥—Ä–∞–Ω–¥–æ–≤\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-01 20:15:33,870 - INFO - üîö –î—Ä–∞–π–≤–µ—Ä –∑–∞–∫—Ä—ã—Ç\n"
     ]
    }
   ],
   "source": [
    "# parsers/news_parser.py\n",
    "\n",
    "import sqlite3\n",
    "import logging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, WebDriverException, TimeoutException\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class NewsParser:\n",
    "    \"\"\"\n",
    "    –ö–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å championat.com\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self.wait = None\n",
    "        self.parsed_news = []  # –•—Ä–∞–Ω–∏–º —Å–ø–∞—Ä—à–µ–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –≤ –ø–∞–º—è—Ç–∏\n",
    "        \n",
    "    def setup_driver(self):\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥—Ä–∞–π–≤–µ—Ä–∞ Chrome\"\"\"\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "        \n",
    "        self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        self.wait = WebDriverWait(self.driver, 10)  # –û–∂–∏–¥–∞–Ω–∏–µ –¥–æ 10 —Å–µ–∫—É–Ω–¥\n",
    "        \n",
    "    def get_category_from_url(self, url):\n",
    "        \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å–ø–æ—Ä—Ç–∞ –∏–∑ URL\"\"\"\n",
    "        try:\n",
    "            parsed_url = urlparse(url)\n",
    "            path_parts = parsed_url.path.strip('/').split('/')\n",
    "            if len(path_parts) > 0:\n",
    "                return path_parts[0]\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ —Å–∏–º–≤–æ–ª–æ–≤\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫\n",
    "        cleaned = re.sub(r'\\s+', ' ', text.strip())\n",
    "        return cleaned\n",
    "\n",
    "    def is_duplicate_news(self, url):\n",
    "        \"\"\"–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–∞—è –Ω–æ–≤–æ—Å—Ç—å –≤ —Å–ø–∏—Å–∫–µ —Å–ø–∞—Ä—à–µ–Ω–Ω—ã—Ö\"\"\"\n",
    "        return any(news['url'] == url for news in self.parsed_news)\n",
    "\n",
    "    def save_news_data(self, news_data):\n",
    "        \"\"\"–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤–æ—Å—Ç—å –≤ —Å–ø–∏—Å–æ–∫ (–ø–æ–∫–∞ –±–µ–∑ –ë–î)\"\"\"\n",
    "        try:\n",
    "            # –î–æ–±–∞–≤–ª—è–µ–º timestamp –ø–∞—Ä—Å–∏–Ω–≥–∞\n",
    "            news_data['parsed_at'] = datetime.now().isoformat()\n",
    "            \n",
    "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å\n",
    "            self.parsed_news.append(news_data)\n",
    "            \n",
    "            logger.info(f\"‚úÖ –ù–æ–≤–æ—Å—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–∞: {news_data['title'][:50]}...\")\n",
    "            logger.info(f\"üìä –í—Å–µ–≥–æ —Å–ø–∞—Ä—à–µ–Ω–æ: {len(self.parsed_news)} –Ω–æ–≤–æ—Å—Ç–µ–π\")\n",
    "            \n",
    "            return len(self.parsed_news) - 1  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–Ω–¥–µ–∫—Å\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_parsed_news(self):\n",
    "        \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ —Å–ø–∞—Ä—à–µ–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏\"\"\"\n",
    "        return self.parsed_news\n",
    "\n",
    "    def export_to_json(self, filename=\"parsed_news.json\"):\n",
    "        \"\"\"–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Å–ø–∞—Ä—à–µ–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –≤ JSON —Ñ–∞–π–ª\"\"\"\n",
    "        import json\n",
    "        \n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.parsed_news, f, ensure_ascii=False, indent=2)\n",
    "            logger.info(f\"üìÅ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {filename}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {e}\")\n",
    "\n",
    "    def print_news_summary(self):\n",
    "        \"\"\"–í—ã–≤–æ–¥–∏—Ç –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É —Å–ø–∞—Ä—à–µ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"üìä –°–í–û–î–ö–ê –ü–ê–†–°–ò–ù–ì–ê: {len(self.parsed_news)} –Ω–æ–≤–æ—Å—Ç–µ–π\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for i, news in enumerate(self.parsed_news, 1):\n",
    "            print(f\"\\n{i}. {news['title']}\")\n",
    "            print(f\"   üîó {news['url']}\")\n",
    "            print(f\"   üìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {news.get('category', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞')}\")\n",
    "            print(f\"   üìù –¢–µ–∫—Å—Ç: {len(news.get('text', ''))} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "            print(f\"   üè∑Ô∏è  –¢–µ–≥–∏: {len(news.get('tags', []))}\")\n",
    "            print(f\"   üñºÔ∏è  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {len(news.get('images', []))}\")\n",
    "            print(f\"   üé¨ –í–∏–¥–µ–æ: {len(news.get('videos', []))}\")\n",
    "            \n",
    "            if news.get('tags'):\n",
    "                print(f\"   –¢–µ–≥–∏: {', '.join([tag['name'] for tag in news['tags'][:3]])}{'...' if len(news['tags']) > 3 else ''}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "    def parse_article_details(self, news_url):\n",
    "        \"\"\"–ü–∞—Ä—Å–∏—Ç –¥–µ—Ç–∞–ª–∏ —Å—Ç–∞—Ç—å–∏ –ø–æ –µ–µ URL\"\"\"\n",
    "        data = {\n",
    "            'text': None,\n",
    "            'tags': [],\n",
    "            'images': [],\n",
    "            'videos': [],\n",
    "            'published_at': None\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            self.driver.get(news_url)\n",
    "            time.sleep(3)  # –î–∞–µ–º –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∏—Ç—å—Å—è\n",
    "            \n",
    "            # --- –ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏ (–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã) ---\n",
    "            text_selectors = [\n",
    "                \"article p\",\n",
    "                \".article-content p\",\n",
    "                \".news-content p\", \n",
    "                \".content p\",\n",
    "                \"[class*='article'] p\",\n",
    "                \".text p\"\n",
    "            ]\n",
    "            \n",
    "            for selector in text_selectors:\n",
    "                try:\n",
    "                    paragraphs = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if paragraphs:\n",
    "                        full_text = \" \".join([p.text for p in paragraphs if p.text.strip()])\n",
    "                        if len(full_text) > 50:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "                            data['text'] = self.clean_text(full_text)\n",
    "                            logger.info(f\"‚úÖ –¢–µ–∫—Å—Ç –Ω–∞–π–¥–µ–Ω —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º: {selector}\")\n",
    "                            break\n",
    "                except NoSuchElementException:\n",
    "                    continue\n",
    "            \n",
    "            # –ï—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥\n",
    "            if not data['text']:\n",
    "                try:\n",
    "                    # –ò—â–µ–º –ª—é–±–æ–π –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ\n",
    "                    all_p = self.driver.find_elements(By.TAG_NAME, \"p\")\n",
    "                    texts = [p.text for p in all_p if len(p.text.strip()) > 30]\n",
    "                    if texts:\n",
    "                        data['text'] = self.clean_text(\" \".join(texts))\n",
    "                        logger.info(\"‚úÖ –¢–µ–∫—Å—Ç –Ω–∞–π–¥–µ–Ω –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º\")\n",
    "                except:\n",
    "                    logger.warning(f\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ç–µ–∫—Å—Ç –¥–ª—è {news_url}\")\n",
    "            \n",
    "            # --- –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ ---\n",
    "            date_selectors = [\n",
    "                \"[datetime]\",\n",
    "                \".article__date\",\n",
    "                \".news-item__date\",\n",
    "                \".date\",\n",
    "                \"[class*='date']\"\n",
    "            ]\n",
    "            \n",
    "            for selector in date_selectors:\n",
    "                try:\n",
    "                    date_element = self.driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                    date_text = date_element.get_attribute(\"datetime\") or date_element.text\n",
    "                    if date_text:\n",
    "                        data['published_at'] = self.parse_date(date_text)\n",
    "                        break\n",
    "                except NoSuchElementException:\n",
    "                    continue\n",
    "            \n",
    "            # --- –ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ–≥–æ–≤ ---\n",
    "            tags_selectors = [\n",
    "                \".article__tags a\",\n",
    "                \".tags__items a\", \n",
    "                \".tags a\",\n",
    "                \"[class*='tag'] a\"\n",
    "            ]\n",
    "            \n",
    "            for selector in tags_selectors:\n",
    "                try:\n",
    "                    tag_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    for tag_el in tag_elements:\n",
    "                        tag_text = self.clean_text(tag_el.text)\n",
    "                        tag_url = tag_el.get_attribute(\"href\")\n",
    "                        \n",
    "                        if tag_text and tag_url and \"—Ç–µ–≥–∏\" not in tag_text.lower():\n",
    "                            data['tags'].append({\n",
    "                                'name': tag_text,\n",
    "                                'url': tag_url\n",
    "                            })\n",
    "                    \n",
    "                    if data['tags']:\n",
    "                        logger.info(f\"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(data['tags'])} —Ç–µ–≥–æ–≤\")\n",
    "                        break\n",
    "                        \n",
    "                except NoSuchElementException:\n",
    "                    continue\n",
    "            \n",
    "            # --- –ü–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ---\n",
    "            try:\n",
    "                img_elements = self.driver.find_elements(By.CSS_SELECTOR, \"article img, .article-content img, .content img\")\n",
    "                \n",
    "                for img in img_elements:\n",
    "                    src = img.get_attribute(\"src\")\n",
    "                    alt = img.get_attribute(\"alt\") or \"\"\n",
    "                    \n",
    "                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "                    if src and not any(skip in src.lower() for skip in ['logo', 'icon', 'avatar', 'btn', 'sprite']):\n",
    "                        data['images'].append({\n",
    "                            'src': src,\n",
    "                            'alt': self.clean_text(alt)\n",
    "                        })\n",
    "                        \n",
    "                if data['images']:\n",
    "                    logger.info(f\"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(data['images'])} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}\")\n",
    "            \n",
    "            # --- –ü–∞—Ä—Å–∏–Ω–≥ –≤–∏–¥–µ–æ ---\n",
    "            try:\n",
    "                iframe_elements = self.driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "                for iframe in iframe_elements:\n",
    "                    src = iframe.get_attribute(\"src\")\n",
    "                    if src and any(domain in src for domain in [\"youtube.com\", \"rutube.ru\", \"vk.com/video\"]):\n",
    "                        data['videos'].append(src)\n",
    "                        \n",
    "                if data['videos']:\n",
    "                    logger.info(f\"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(data['videos'])} –≤–∏–¥–µ–æ\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–∏–¥–µ–æ: {e}\")\n",
    "                \n",
    "        except TimeoutException:\n",
    "            logger.error(f\"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {news_url}\")\n",
    "        except WebDriverException as e:\n",
    "            logger.error(f\"–û—à–∏–±–∫–∞ WebDriver –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {news_url}: {e}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {news_url}: {e}\")\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def parse_date(self, date_string):\n",
    "        \"\"\"–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤\"\"\"\n",
    "        if not date_string:\n",
    "            return None\n",
    "            \n",
    "        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç\n",
    "        date_formats = [\n",
    "            \"%Y-%m-%dT%H:%M:%S\",\n",
    "            \"%Y-%m-%d %H:%M\",\n",
    "            \"%d.%m.%Y %H:%M\",\n",
    "            \"%d.%m.%Y\"\n",
    "        ]\n",
    "        \n",
    "        for fmt in date_formats:\n",
    "            try:\n",
    "                return datetime.strptime(date_string.strip(), fmt).isoformat()\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        logger.warning(f\"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É: {date_string}\")\n",
    "        return None\n",
    "\n",
    "    def parse_news_page(self, page=1, limit=None):\n",
    "        \"\"\"–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–æ–≤–æ—Å—Ç–µ–π\"\"\"\n",
    "        url = f\"https://www.championat.com/news/{page}.html\"\n",
    "        logger.info(f\"–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}\")\n",
    "        \n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            time.sleep(3)  # –î–∞–µ–º –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∏—Ç—å—Å—è\n",
    "            \n",
    "            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏ —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏\n",
    "            posts = []\n",
    "            selectors = [\n",
    "                \".page-content .news-item\",\n",
    "                \".news-item\",\n",
    "                \".news-list .news-item\",\n",
    "                \"[class*='news-item']\"\n",
    "            ]\n",
    "            \n",
    "            for selector in selectors:\n",
    "                try:\n",
    "                    posts = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if posts:\n",
    "                        logger.info(f\"‚úÖ –ù–∞–π–¥–µ–Ω—ã –Ω–æ–≤–æ—Å—Ç–∏ —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º: {selector}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if not posts:\n",
    "                logger.error(\"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ\")\n",
    "                return\n",
    "                \n",
    "            if limit:\n",
    "                posts = posts[:limit]\n",
    "            \n",
    "            logger.info(f\"–ù–∞–π–¥–µ–Ω–æ {len(posts)} –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ\")\n",
    "            \n",
    "            # –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ (–∏–∑–±–µ–≥–∞–µ–º stale elements)\n",
    "            news_links = []\n",
    "            for i, post in enumerate(posts, 1):\n",
    "                try:\n",
    "                    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Å—Å—ã–ª–æ–∫\n",
    "                    link_selectors = [\n",
    "                        \".news-item__content a\",\n",
    "                        \".news-item__title a\", \n",
    "                        \"a[href*='/news/']\",\n",
    "                        \"h3 a\", \"h2 a\"\n",
    "                    ]\n",
    "                    \n",
    "                    link_element = None\n",
    "                    for sel in link_selectors:\n",
    "                        try:\n",
    "                            link_element = post.find_element(By.CSS_SELECTOR, sel)\n",
    "                            break\n",
    "                        except NoSuchElementException:\n",
    "                            continue\n",
    "                    \n",
    "                    if link_element:\n",
    "                        link = link_element.get_attribute(\"href\")\n",
    "                        title = self.clean_text(link_element.text)\n",
    "                        \n",
    "                        if link and title:\n",
    "                            news_links.append({\n",
    "                                'link': link,\n",
    "                                'title': title,\n",
    "                                'category': self.get_category_from_url(link)\n",
    "                            })\n",
    "                            logger.info(f\"[{i}/{len(posts)}] –ù–∞–π–¥–µ–Ω–∞: {title[:50]}...\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–æ—Å—Ç–∞ {i}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"–°–æ–±—Ä–∞–Ω–æ {len(news_links)} —Å—Å—ã–ª–æ–∫ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞\")\n",
    "            \n",
    "            # –¢–µ–ø–µ—Ä—å –ø–∞—Ä—Å–∏–º –∫–∞–∂–¥—É—é –Ω–æ–≤–æ—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ\n",
    "            parsed_count = 0\n",
    "            skipped_count = 0\n",
    "            \n",
    "            for i, news_item in enumerate(news_links, 1):\n",
    "                try:\n",
    "                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã\n",
    "                    if self.is_duplicate_news(news_item['link']):\n",
    "                        logger.info(f\"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç: {news_item['title'][:50]}...\")\n",
    "                        skipped_count += 1\n",
    "                        continue\n",
    "                    \n",
    "                    logger.info(f\"[{i}/{len(news_links)}] –ü–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª–∏: {news_item['title'][:50]}...\")\n",
    "                    \n",
    "                    # –ü–∞—Ä—Å–∏–º –¥–µ—Ç–∞–ª–∏ —Å—Ç–∞—Ç—å–∏\n",
    "                    article_data = self.parse_article_details(news_item['link'])\n",
    "                    \n",
    "                    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\n",
    "                    article_data.update(news_item)\n",
    "                    \n",
    "                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "                    if article_data['text']:\n",
    "                        self.save_news_data(article_data)\n",
    "                        parsed_count += 1\n",
    "                    else:\n",
    "                        logger.warning(f\"–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è: {news_item['title']}\")\n",
    "                    \n",
    "                    time.sleep(2)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –Ω–æ–≤–æ—Å—Ç–∏ {i}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"–ó–∞–≤–µ—Ä—à–µ–Ω–æ. –°–ø–∞—Ä—à–µ–Ω–æ: {parsed_count}, –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_count}\")\n",
    "            \n",
    "            # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É\n",
    "            if parsed_count > 0:\n",
    "                self.print_news_summary()\n",
    "            \n",
    "        except TimeoutException:\n",
    "            logger.error(\"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–æ–≤–æ—Å—Ç–µ–π\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"–û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}\")\n",
    "\n",
    "    def run(self, pages=1, limit_per_page=None):\n",
    "        \"\"\"–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞\"\"\"\n",
    "        try:\n",
    "            self.setup_driver()\n",
    "            logger.info(\"üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π championat.com\")\n",
    "            \n",
    "            for page in range(1, pages + 1):\n",
    "                logger.info(f\"üìÑ –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}\")\n",
    "                self.parse_news_page(page, limit_per_page)\n",
    "                \n",
    "                if page < pages:\n",
    "                    time.sleep(3)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏\n",
    "            \n",
    "            # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "            logger.info(f\"üéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω! –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(self.parsed_news)}\")\n",
    "            \n",
    "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "            if self.parsed_news:\n",
    "                self.export_to_json()\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"–ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}\")\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                logger.info(\"üîö –î—Ä–∞–π–≤–µ—Ä –∑–∞–∫—Ä—ã—Ç\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏\n",
    "def main():\n",
    "    parser = NewsParser()\n",
    "    \n",
    "    # –ú–æ–∂–µ—à—å –º–µ–Ω—è—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:\n",
    "    # parser.run(pages=1, limit_per_page=3)  # 3 –Ω–æ–≤–æ—Å—Ç–∏ —Å 1 —Å—Ç—Ä–∞–Ω–∏—Ü—ã\n",
    "    # parser.run(pages=2, limit_per_page=5)  # 5 –Ω–æ–≤–æ—Å—Ç–µ–π —Å 2 —Å—Ç—Ä–∞–Ω–∏—Ü\n",
    "    \n",
    "    parser.run(pages=1, limit_per_page=5)  # –ü–∞—Ä—Å–∏–º 5 –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
